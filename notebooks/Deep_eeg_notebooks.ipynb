{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-eeg-notebooks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylemath/eeg-notebooks/blob/master/notebooks/Deep_eeg_notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cH7KRd8ZZPMd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep-eeg-notebooks\n",
        "\n",
        "Goal is to make deep learning stacks that processes Muse eeg-notebook EEG trials as input and predicts binary trial category as output. This is all made to run on Google Colab notebooks using cloud GPU capabilities, so the git repo's get loaded at the start of the code into the workspace. Minor mods may be needed to use local Jupyter notebook. Long term goal of command line interface and mne toolbox.\n",
        "\n",
        "Strategy:\n",
        "* Load in Muse data, normal pre-processing to get to trials\n",
        "* Options for frequency or time domain\n",
        "* Decimate time to reduce features\n",
        "* Dataset example: Predict attend left vs attend right on muse cueing data\n",
        "\n",
        "API:\n",
        "* Input the data directory and subject numbers of any eeg-notebook experiment (https://github.com/kylemath/eeg-notebooks)\n",
        "* More general with the master eeg-notebooks - https://github.com/NeuroTechX/eeg-notebooks\n",
        "\n",
        "LearningModels:\n",
        "* First try basic Neural Network (NN)\n",
        "* Then try Convolution Neural Net (CNN)\n",
        "* Then try Long-Short Term Memory Recurrant Neural Net (LSTM, RNN)\n",
        "\n",
        "DataModels:\n",
        "* First pool data over all subjects\n",
        "* Try subject specific models\n",
        "* Then try multilevel models\n",
        "\n",
        "Using: \n",
        "* https://github.com/kylemath/eeg-notebooks\n",
        "* https://github.com/mne-tools/mne-python\n",
        "* https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n",
        "* https://github.com/ml4a/ml4a-guides/blob/master/notebooks/keras_classification.ipynb\n",
        "* https://github.com/tevisgehr/EEG-Classification\n",
        "\n",
        "Resources:\n",
        "* https://arxiv.org/pdf/1901.05498.pdf \n",
        "* http://proceedings.mlr.press/v56/Thodoroff16.pdf\n",
        "*  https://arxiv.org/abs/1511.06448\n",
        "*  https://github.com/ml4a\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Kf-1ItUyU380",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ]
    },
    {
      "metadata": {
        "id": "VXe-o8XtG9ki",
        "colab_type": "code",
        "outputId": "d3ad5fbf-907c-4a1b-e965-6cc9821da638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install mne\n",
        "from mne import Epochs, find_events, concatenate_raws\n",
        "from mne.time_frequency import tfr_morlet\n",
        "\n",
        "!git clone https://github.com/kylemath/eeg-notebooks.git\n",
        "%cd eeg-notebooks/notebooks\n",
        "from utils import utils\n",
        "\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.precision = 4\n",
        "\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation \n",
        "from keras.layers import Flatten, Conv2D, MaxPooling2D, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#find the factors of a number |to add extra dimension for CNN|\n",
        "def factors(n):\n",
        "      return [i for i in range(1, n + 1) if not n%i]\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.17.0)\n",
            "Cloning into 'eeg-notebooks'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
            "remote: Total 1748 (delta 133), reused 143 (delta 43), pack-reused 1466\u001b[K\n",
            "Receiving objects: 100% (1748/1748), 108.41 MiB | 18.09 MiB/s, done.\n",
            "Resolving deltas: 100% (756/756), done.\n",
            "Checking out files: 100% (485/485), done.\n",
            "/content/eeg-notebooks/notebooks/eeg-notebooks/notebooks/eeg-notebooks/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yR1WkVZKU2Vm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ]
    },
    {
      "metadata": {
        "id": "QGVIiUbAU1nA",
        "colab_type": "code",
        "outputId": "9e3979d7-4dc3-4576-bb5d-3f5f92ee40ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "#Parameters#\n",
        "#subject folders in data folder\n",
        "subs = [101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112,\n",
        "        202, 203, 204, 205, 207, 208, 209, 210, 211, \n",
        "        301, 302, 303, 304, 305, 306, 307, 308, 309]\n",
        "\n",
        "#for testing\n",
        "#subs = [ 205, 207, 208, 209]\n",
        "subs = [1]\n",
        "#number of sessions in each subject folder\n",
        "nsesh = 1\n",
        "\n",
        "data_dir = 'visual/cueing'\n",
        "event_ids = ['LeftCue','RightCue']\n",
        "\n",
        "\n",
        "## Can load other experiments like this\n",
        "#subs = [ 1]\n",
        "#nsesh = 1\n",
        "#data_dir = 'visual/P300'\n",
        "#event_ids = ['Standard','Target']\n",
        "##\n",
        "\n",
        "## Can load other experiments like this\n",
        "subs = [ 4]\n",
        "nsesh = 1\n",
        "data_dir = 'visual/SSVEP'\n",
        "event_ids = ['30Hz','20Hz']\n",
        "##\n",
        "\n",
        "\n",
        "load_verbose = 0 #print output during loading\n",
        "\n",
        "sfreq=256.\n",
        "decim=2  # to decrease number of time features \n",
        "nsfreq = sfreq/decim\n",
        "\n",
        "#filter\n",
        "eeg_filter_highpass = 1\n",
        "eeg_filter_lowpass = nsfreq/2.5  #lower to avoid aliasing from decim\n",
        "\n",
        "#artifact rejection\n",
        "rej_thresh_uV = 10000 #300 removes most blinks, 1000 keeps most trials\n",
        "rej_thresh = rej_thresh_uV*1e-6\n",
        "\n",
        "#trial epoch limits \n",
        "tmin=-1 #-1\n",
        "tmax=4 #2\n",
        "baseline=(-1, 0) #None or (start, end) in seconds\n",
        "#gets removed from training data but used during processing\n",
        "\n",
        "#wavelet settings\n",
        "f_low = 4\n",
        "f_high = 40\n",
        "f_bins = 49  # to decrease number of frequency features\n",
        "wave_cycles = 6 # cycles of each frequency to convolve with data\n",
        "spect_baseline = [-1,-.5] #gets removed from training data but used during processing\n",
        "electrodes_out = [4]  # to decrease number of electrode features from [0,1,2,3] or [0,1,2,3,4] with aux electrode (untested)\n",
        "frequencies =  np.linspace(f_low, f_high, f_bins, endpoint=True)\n",
        "\n",
        "frequency_domain = 0  #if True uses wavelets otherwise use time series\n",
        "\n",
        "#Training Settings\n",
        "batch_size = 10\n",
        "train_epochs = 20\n",
        "num_classes = len(event_ids)\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "random_seed = 1017\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# 'CNN','LSTM','NN'\n",
        "model_type = 'LSTM'\n",
        "\n",
        "print('Parameters Set')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters Set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2a98-rwL9wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preprocessing of EEG Data**\n",
        "\n",
        "* LoadData - Each data file is from one session of one subject of the muse cueing task from eeg-notebooks from psych 375 A1 dataset\n",
        "* Filtering - Most ERP components are composed of lower frequency fluctuations in the EEG signal. Thus, we can filter out all frequencies between 1 and 30 hz in order to increase our ability to detect them.\n",
        "* Epoching - Next, we will chunk (epoch) the data into segments representing the data 1000ms before to 2000ms after each cue, we will reject every epoch where the amplitude of the signal exceeded 100 uV, which should most eye blinks."
      ]
    },
    {
      "metadata": {
        "id": "6A3mgX7ILZFK",
        "colab_type": "code",
        "outputId": "f8ef93b6-20f2-4fc2-95f3-c89de2e38fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "cell_type": "code",
      "source": [
        "#Load Data\n",
        "print('Loading Data')\n",
        "isub = subs[0]\n",
        "print(isub)\n",
        "raw = utils.load_data(data_dir, sfreq=sfreq ,subject_nb=isub, session_nb=1,verbose=load_verbose)\n",
        "if nsesh == 2:\n",
        "  raw.append(utils.load_data(data_dir, sfreq=sfreq,subject_nb=isub, session_nb=2,verbose=load_verbose))\n",
        "\n",
        "if len(subs)>1:\n",
        "  for isub in subs[1:]:\n",
        "    print(isub)\n",
        "    raw.append(utils.load_data(data_dir, sfreq=sfreq,subject_nb=isub, session_nb=1,verbose=load_verbose))\n",
        "    if nsesh == 2:\n",
        "      raw.append(utils.load_data(data_dir, sfreq=sfreq,subject_nb=isub, session_nb=2,verbose=load_verbose))\n",
        "  \n",
        "#Filtering\n",
        "print('Filtering Data')\n",
        "raw.filter(eeg_filter_highpass,eeg_filter_lowpass, method='iir', verbose='WARNING' )\n",
        "\n",
        "#Plot filtred data spectra if desired\n",
        "#raw.plot_psd(fmin=eeg_filter_highpass, fmax=eeg_filter_lowpass ) \n",
        "\n",
        "#Epoching\n",
        "event_id = {event_ids[0]: 1, event_ids[1]: 2}\n",
        "events = find_events(raw)\n",
        "epochs = Epochs(raw, events=events, event_id=event_id, \n",
        "                tmin=tmin, tmax=tmax, baseline=baseline, \n",
        "                preload=True,reject={'eeg':rej_thresh},\n",
        "                verbose=False, decim=decim)\n",
        "\n",
        "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
        "%matplotlib inline\n",
        "conditions = OrderedDict()\n",
        "conditions[event_ids[0]] = [1]\n",
        "conditions[event_ids[1]] = [2]\n",
        "\n",
        "#plot ERPs if desired\n",
        "#fig, ax = utils.plot_conditions(epochs, conditions=conditions, ylim=(-20,20) )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data\n",
            "4\n",
            "Filtering Data\n",
            "168 events found\n",
            "Event IDs: [1 2]\n",
            "sample drop %:  9.523809523809524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmvSu1bOZiO0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Spectrogram **\n",
        "\n",
        "* All electrodes are considered on each trial\n",
        "* Many frequencies are used in the wavelet analysis while the output of the morlet is decimated in time.\n",
        "* Two conditions are done separate and concatenated\n",
        "* X is a trials x freq x time matrix\n",
        "* Y is a trials array of condition labels (turned into one hot vectors later in code)\n"
      ]
    },
    {
      "metadata": {
        "id": "Xs-_8GRYZar7",
        "colab_type": "code",
        "outputId": "dde08f81-3bf3-493a-bd37-c3b52c93d8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "cell_type": "code",
      "source": [
        "if frequency_domain:\n",
        "  # Condition0\n",
        "  print('Computing Morlet Wavelets on ' + event_ids[0])\n",
        "\n",
        "  tfr0 = tfr_morlet(epochs[event_ids[0]], freqs=frequencies, \n",
        "                        n_cycles=wave_cycles, return_itc=False,\n",
        "                        picks=electrodes_out,average=False,decim=decim)\n",
        "  tfr0 = tfr0.apply_baseline(spect_baseline,mode='mean')\n",
        "  stim_onset = np.argmax(tfr0.times>0)\n",
        "  new_times = tfr0.times[stim_onset:]\n",
        "\n",
        "  #reshape data\n",
        "  cond0_power_out = np.moveaxis(tfr0.data[:,:,:,stim_onset:],1,3) #move electrodes last\n",
        "  cond0_power_out = np.moveaxis(cond0_power_out,1,2) # move time second\n",
        "  #cond0_power_out[:,0:5,0:5,:] = 0 #for testing model add mark to image\n",
        "\n",
        "  print('Condition zero trials: ' + str(len(cond0_power_out)))\n",
        "\n",
        "  print(event_ids[0] + ' Time Points: ' + str(len(new_times)))\n",
        "  print(event_ids[0] + ' Frequencies: ' + str(len(tfr0.freqs)))\n",
        "\n",
        "  #Condition1\n",
        "  print('Computing Morlet Wavelets on ' + event_ids[1])\n",
        "  tfr1 = tfr_morlet(epochs[event_ids[1]], freqs=frequencies, \n",
        "                        n_cycles=wave_cycles, return_itc=False,\n",
        "                        picks=electrodes_out,average=False,decim=decim)\n",
        "  tfr1 = tfr1.apply_baseline(spect_baseline,mode='mean')\n",
        "  \n",
        "  #reshape data\n",
        "  cond1_power_out = np.moveaxis(tfr1.data[:,:,:,stim_onset:],1,3)\n",
        "  cond1_power_out = np.moveaxis(cond1_power_out,1,2) # move time second\n",
        "  #cond1_power_out[:,0:5,0:5,:] = 1 #for testing model add mark to image\n",
        "\n",
        "  print('Condition one trials: ' + str(len(cond1_power_out)))    \n",
        "\n",
        "  print(event_ids[1] + ' Time Points: ' + str(len(new_times)))\n",
        "  print(event_ids[1] + ' Frequencies: ' + str(len(tfr1.freqs)))\n",
        "  X = np.append(cond0_power_out,cond1_power_out,0);\n",
        "  \n",
        "  if model_type != 'CNN':\n",
        "    #reshape to trials x times x variables for LSTM and NN model\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], X.shape[2] * X.shape[3]),order='F')\n",
        "   \n",
        "  #Normalize X - need to save mean and std for future test + val\n",
        "  X = (X - np.mean(X)) / np.std(X)\n",
        "  \n",
        "  #Append Data\n",
        "  Y_class = np.append(np.zeros(len(cond0_power_out)), np.ones(len(cond1_power_out)),0)\n",
        "  \n",
        "  print('Combined X Shape: ' + str(X.shape))\n",
        "  print('Combined Y Shape: ' + str(Y_class.shape))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing Morlet Wavelets on 30Hz\n",
            "Applying baseline correction (mode: mean)\n",
            "Condition zero trials: 82\n",
            "30Hz Time Points: 256\n",
            "30Hz Frequencies: 49\n",
            "Computing Morlet Wavelets on 20Hz\n",
            "Applying baseline correction (mode: mean)\n",
            "Condition one trials: 70\n",
            "20Hz Time Points: 256\n",
            "20Hz Frequencies: 49\n",
            "Combined X Shape: (152, 256, 49)\n",
            "Combined Y Shape: (152,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_d74N_oqMlGF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Time Domain**\n",
        "\n",
        "* Instead may want to retain time domain data\n",
        "* After Epoching, arange data into useful format with labels"
      ]
    },
    {
      "metadata": {
        "id": "qoBQKZ2uMOpL",
        "colab_type": "code",
        "outputId": "e16870cd-795b-4663-8479-c5e6dfd56ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "cell_type": "code",
      "source": [
        "if not frequency_domain:\n",
        "  \n",
        "  #epochs = epochs.decimate(10) #probably don't do this\n",
        "  X = np.moveaxis(epochs._data,1,2); #but channels last\n",
        "  \n",
        "  #take post baseline only\n",
        "  stim_onset = np.argmax(epochs.times>0)\n",
        "  new_times = epochs.times[stim_onset:]\n",
        "  X = X[:,stim_onset:,:]\n",
        "  \n",
        "  print(X.shape)\n",
        "  if model_type == 'CNN' and not frequency_domain:\n",
        "    # reshape for CNN 512 ms long, factors 64 * 8\n",
        "    # these 10's need to be found algorithmically (find any factor of the number)\n",
        "    all_factors = factors(X.shape[1])\n",
        "    X = np.reshape(X, (X.shape[0], int(X.shape[1]/all_factors[3]), all_factors[3], X.shape[2]),order='F')\n",
        "  \n",
        "  #Normalize X - need to save mean and std for future test + val\n",
        "  X = (X - np.mean(X)) / np.std(X)\n",
        "  \n",
        "  Y_class = epochs.events[:,2]-1  #subtract 1 to make 0 and 1\n",
        "  \n",
        "  print('X Shape: ' + str(X.shape))\n",
        "  print('Y Shape: ' + str(Y_class.shape))\n",
        "  print('Y Example: ' + str(Y_class[0:10]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(152, 512, 5)\n",
            "X Shape: (152, 512, 5)\n",
            "Y Shape: (152,)\n",
            "Y Example: [0 0 0 0 1 0 1 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WqVTXiwrZxbf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Model**\n"
      ]
    },
    {
      "metadata": {
        "id": "4xyrsW2kZW8D",
        "colab_type": "code",
        "outputId": "b0e834c9-37d5-4f15-ad9c-1fd808cfc9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "cell_type": "code",
      "source": [
        "# convert class vectors to one hot Y and recast X\n",
        "Y = keras.utils.to_categorical(Y_class, num_classes)\n",
        "X = X.astype('float32')\n",
        "\n",
        "# Split training test and validation data \n",
        "val_prop = val_split / (1-test_split)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_split,random_state=random_seed) \n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_prop, random_state=random_seed)\n",
        "\n",
        "# Compute model input shape\n",
        "input_shape = X.shape[1:]\n",
        "\n",
        "\n",
        "print('X Range: ' + str(np.min(X)) + ':' + str(np.max(X)))\n",
        "print('X Examples:')\n",
        "\n",
        "#for plotting\n",
        "\n",
        "vmin = (np.std(X)/4) * -1\n",
        "vmax = np.std(X)/4  \n",
        "\n",
        "#Plot Example Data\n",
        "#f, axarr = plt.subplots(2,2, figsize = (8,8))\n",
        "#axarr[0][0].set_title(event_ids[0])\n",
        "#axarr[0][0].imshow(X[0],vmin=vmin, vmax=vmax, aspect='auto')\n",
        "\n",
        "#axarr[0][0].invert_yaxis()\n",
        "#axarr[1][0].imshow(X[1],vmin=vmin, vmax=vmax, aspect='auto')\n",
        "#axarr[1][0].invert_yaxis()\n",
        "\n",
        "#axarr[0][1].set_title(event_ids[1])\n",
        "#axarr[0][1].imshow(X[-1],vmin=vmin, vmax=vmax, aspect='auto')\n",
        "#axarr[0][1].invert_yaxis()\n",
        "#axarr[1][1].imshow(X[-2],vmin=vmin, vmax=vmax, aspect='auto')\n",
        "#axarr[1][1].invert_yaxis()\n",
        "#;\n",
        "\n",
        "print('Input Shape: ' + str(input_shape))\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print(x_val.shape[0], 'validation samples')\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Range: -0.029341102:89.56634\n",
            "X Examples:\n",
            "Input Shape: (512, 5)\n",
            "x_train shape: (90, 512, 5)\n",
            "90 train samples\n",
            "31 test samples\n",
            "31 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKzYDoF1TQmK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Build Model and Train**"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "JxjbgyC5G7AZ",
        "colab_type": "code",
        "outputId": "39061933-a967-4526-9281-f6a80f2ff754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "print('Running ' +  model_type + ' Model:')\n",
        "if model_type == 'LSTM':\n",
        "  ##---LSTM - Many to two, sequence of time to classes\n",
        "  units = [input_shape[1], 4, 4, num_classes]\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(input_shape=(None, units[0]) ,units=units[1], return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=units[2],return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=units[3]))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "if model_type == 'NN':\n",
        "  ##---DenseFeedforward Network\n",
        "  model = Sequential()\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4, activation='relu'))\n",
        "  model.add(Dropout(.20))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "if model_type == 'CNN':\n",
        "  ##----Convolutional Network\n",
        "  # parameters above need to be adjusted until input shape is square on \n",
        "  # dimensions 1 and 2 (like an image) and electrodes on third\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(49, (3, 3), input_shape=input_shape))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "# initiate adam optimizer\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, \n",
        "                            epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Train Model\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=train_epochs,\n",
        "          validation_data=(x_val, y_val),\n",
        "          shuffle=True,\n",
        "          verbose=True)\n",
        "         \n",
        "#Summarize\n",
        "model.summary()\n",
        " \n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(np.log(history.history['loss']))\n",
        "plt.plot(np.log(history.history['val_loss']))\n",
        "plt.title('model loss')\n",
        "plt.ylabel('log(loss)')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Test on left out Test data\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running LSTM Model:\n",
            "Train on 90 samples, validate on 31 samples\n",
            "Epoch 1/20\n",
            "90/90 [==============================] - 21s 238ms/step - loss: 0.6928 - acc: 0.5778 - val_loss: 0.6934 - val_acc: 0.4839\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 17s 187ms/step - loss: 0.6909 - acc: 0.5556 - val_loss: 0.6942 - val_acc: 0.4839\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 17s 188ms/step - loss: 0.6901 - acc: 0.5556 - val_loss: 0.6953 - val_acc: 0.4839\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 16s 183ms/step - loss: 0.6922 - acc: 0.5556 - val_loss: 0.6960 - val_acc: 0.4839\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 16s 183ms/step - loss: 0.6889 - acc: 0.5556 - val_loss: 0.6962 - val_acc: 0.4839\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 17s 186ms/step - loss: 0.6894 - acc: 0.5556 - val_loss: 0.6972 - val_acc: 0.4839\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 18s 200ms/step - loss: 0.6905 - acc: 0.5556 - val_loss: 0.6978 - val_acc: 0.4839\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 17s 186ms/step - loss: 0.6869 - acc: 0.5556 - val_loss: 0.6988 - val_acc: 0.4839\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 17s 186ms/step - loss: 0.6820 - acc: 0.5556 - val_loss: 0.6998 - val_acc: 0.4839\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 17s 188ms/step - loss: 0.6906 - acc: 0.5556 - val_loss: 0.6999 - val_acc: 0.4839\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 17s 190ms/step - loss: 0.6890 - acc: 0.5556 - val_loss: 0.6999 - val_acc: 0.4839\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 17s 189ms/step - loss: 0.6853 - acc: 0.5556 - val_loss: 0.7010 - val_acc: 0.4839\n",
            "Epoch 13/20\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}